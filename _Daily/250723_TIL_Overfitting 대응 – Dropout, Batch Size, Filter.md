# Overfitting 대응 – Dropout, Batch Size, Filter

## Dropout Rate 조절 ⭐⭐⭐

- **정의**: Dropout은 학습 시 뉴런을 임의로 비활성화하여 특정 패턴 의존을 줄이는 정규화 기법.

- **중요 이유**: Dropout 비율을 높이면(random 비활성화) 모델의 복잡도 감소 → 과적합 현상(새로운 데이터 일반화 실패) 완화에 효과적.

- **핵심 요소**:

- 비율 증가 효과: dropout_rate를 0.2→0.5로 올리면 regularization이 강화됨.

- 작동 원리: 학습마다 무작위로 뉴런을 off, 앙상블 효과로 모델 일반화 성능 개선.

- **연결**: → Filter 수 감소와 병행 시 구조적 복잡도 조정 가능.

## Batch Size 조정 ⭐⭐

- **개요**: Batch size는 파라미터가 업데이트되는 미니 배치 데이터의 크기로, 일반화와 학습 효율에 영향.

- **주요 특징**:

- 크기 증가 시: 더 평탄한 gradient 사용 → 데이터 특이치 민감도 감소, 초과 과적합 방지에 기여할 수 있음.

- 실제 활용: 32→128 등 단계적 증대로 적정 범위 탐색(과대 설정 시 오히려 일반화 악화).

- **주의점**: 지나치게 크면 학습 속도는 빨라지나 모델이 데이터 분포 세부 특성 파악에 실패할 수 있음.

## Conv2D Filter 값 줄이기 ⭐

- **요약**: 필터 개수를 줄이면 파라미터나 표현력 감소로 과적합 위험 줄어듦.

- **활용**: 초기 filter1,2를 64→32 등으로 조정해 검증 성능관찰, 필요시 재조정.

- **참고**: 너무 적게 줄일 경우 주요 feature 추출 성능 저하될 수 있음.

## 핵심 연결고리

- Dropout 증가, Filter 감소 → 모델 복잡도 구조적 축소 → 과적합 저감

- Batch Size 증가는 학습 과정의 노이즈/평균값 조정 → 일반화 성능 유동적 변화

- 전체 흐름: 하이퍼파라미터별 조정은 과적합 완화와 성능 저하(또는 과소적합) 위험 사이 trade-off 필요. 실험적으로 한 번에 하나씩 변경, 검증 데이터 기반으로 판단할 것.

- Dropout과 Filter 조정은 네트워크 구조 자체의 regularization 강화를, Batch Size 조정은 학습 프로세스 관점의 일반화 강화를 의미함.

- 모든 파라미터 조정은 실험적 튜닝과 검증 셋 성능 관찰이 필수적임.