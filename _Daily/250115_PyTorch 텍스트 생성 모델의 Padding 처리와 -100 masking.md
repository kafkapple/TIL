### **1. 문제 배경**
- 텍스트 생성 모델은 입력 시퀀스를 기반으로 출력 시퀀스를 생성함.
- 배치 내 시퀀스 길이 불일치 해결을 위해 **패딩(padding)** 필요.
---
### **2. 패딩과 문제점**
- 패딩은 학습과 무관하므로 **loss 계산에서 제외** 필요.
- PyTorch의 `CrossEntropyLoss`는 `ignore_index=-100`으로 설정하여 특정 값을 손실 계산에서 무시.
---
### **3. 마스킹 처리 과정**
- 입력 텐서와 레이블 텐서의 패딩 영역을 `100`으로 마스킹.
- 예시:
    ```
    레이블 텐서: [[1045, 2572, 2986, 1010, 4283, 1012, -100],
                 [2025, 2172, 1012, -100, -100, -100, -100, -100]]
    ```
---
### **4. 손실 계산 방식**
- 손실 계산은 `ignore_index=-100` 설정에 따라 패딩 영역을 무시:
---
### **5. 주요 이점**
1. **패딩 토큰 무시로 학습 안정성 증가**: 불필요한 잡음 방지.
2. **배치 크기 유연성 제공**: 샘플 길이 불일치 해결.
3. **프레임워크 지원**: PyTorch와 HuggingFace 기본 적용.
---
### **6. 활용 시나리오**
- **학습 시**: `100`으로 마스킹하여 특정 영역(loss 제외)을 예측.
- **평가 시**: 전체를 `100`으로 설정해 모델이 자유롭게 생성.
---
### **결론**
- `100` 마스킹은 텍스트 생성 모델 학습에서 필수적인 전처리 방식.
- PyTorch 및 HuggingFace는 이를 효율적으로 지원하며, 텍스트 생성 모델의 손실 계산 최적화에 중요한 역할을 함.