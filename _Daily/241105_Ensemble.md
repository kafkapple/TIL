# I. 머신러닝 실험 및 앙상블 학습 팁 요약 (2024-11-05)
---
## 1. 디버깅 모드
실험 환경 설정 및 검증을 위해 디버깅 모드를 활용:
- **프로세스 축소**:
    - 샘플 데이터로 테스트.
    - 작은 규모의 모델 사용.
    - 하이퍼파라미터 수 축소.
    - 랜덤 시드 고정으로 재현성 확보.
---
## 2. 실험 기록
- 조건을 하나씩 변경하며 실험 진행 권장.
- 결과를 체계적으로 기록해 비교.
---
## 3. 앙상블 학습 전략
### **데이터 기반 앙상블**
- **K-fold 교차 검증**으로 데이터 다변화.
### **모델 기반 앙상블**
- **LightGBM**과 **CatBoost** 조합 권장.
- **Linear Regression**과 **Boosting** 각각의 장점을 확인.
- **Boosting**과 **Neural Network**를 결합하는 경우도 존재.
### **성능 요건**:
- 개별 모델의 기본 성능이 어느 정도 보장되어야 함.
- 상관 관계가 낮은(서로 다른) 모델을 결합해 성능 향상.
### **Stacking**:
- 각 모델의 예측 결과를 새로운 피처로 사용하여 추가 학습.
- 대회에서 종종 활용되지만, 비선호하는 경우도 있음.
### **랜덤성 활용**:
- 앙상블 과정에서 랜덤성을 적절히 활용하여 다양성을 확보.
---
## 4. 기타
- 앙상블에 들어가는 모델은 각기 다른 특징과 장점을 가져야 효과적.
# II. Study
## 1. 앙상블 학습 개요
앙상블 학습은 여러 모델의 예측 결과를 결합하여 일반화 성능을 높이는 기법으로, 주로 Bagging과 Boosting 두 가지 방식이 있다.
### 주요 개념
- **Bootstrap**:
    - 복원 추출을 사용하여 표본을 생성하고 모집단 통계량을 추론하는 통계적 방법.
    - 복원 추출을 통해 각 표본이 독립적으로 구성됨.
- **Tree Ensemble**:
    - Bagging과 Boosting을 기반으로 한 결정 트리 기반 앙상블 기법.
- **결과 결합 방법**:
    - **Voting**:
        - 분류 문제(categorical)에서 다수결로 최종 결과 결정.
    - **Average**:
        - 회귀 문제(continuous)에서 평균값으로 최종 결과 계산.
---
## 2. Bagging (Bootstrap + Aggregation)
- **정의**:복원 추출로 생성된 여러 데이터 서브셋을 독립적으로 학습시키고 결과를 결합하는 방식.
- **작동 원리**:
    1. Bootstrap 샘플링으로 데이터 서브셋 생성.
    2. 각 서브셋을 독립적인 모델로 학습.
    3. 결과를 Aggregation(voting/average)하여 최종 예측 생성.
- **대표 모델**:**랜덤 포레스트(Random Forest)**.
- **장점**:
    - 분산 감소로 일반화 성능 향상.
    - 과적합 완화.
---
## 3. Boosting
- **정의**:이전 모델의 예측 오류를 개선하는 방식으로 모델을 순차적으로 학습하며 성능을 점진적으로 향상.
- **작동 원리**:
    1. 첫 모델 학습 후 예측 오류(Residual)를 계산.
    2. 오류를 개선하는 방향으로 다음 모델 학습.
    3. 가중치를 조정하여 최종 예측 생성.
- **대표 알고리즘**:
    - **에이다부스트(AdaBoost)**:이전 모델의 오류에 가중치를 부여해 다음 모델 학습에 반영.
    - **그라디언트 부스팅 머신(GBM)**:손실 함수의 그래디언트에 따라 잔차를 줄이는 방향으로 학습.
        - **XGBoost**: GBM의 개선형으로 학습 속도와 정확도 향상.
        - **LightGBM**: 대규모 데이터셋에 최적화.
        - **CatBoost**: 범주형 데이터 처리에 특화.
- **장점**:
    - 높은 예측 정확도.
    - 점진적 성능 향상.
- **단점**:
    - 계산 비용이 높음.
    - 과적합 가능성.
---
## 4. Bagging vs. Boosting 비교
| 특징 | Bagging | Boosting |
| --- | --- | --- |
| 데이터 구성 | 독립적 표본 구성 (복원 추출) | 이전 모델 오류를 기반으로 구성 |
| 모델 학습 | 각 모델 독립적으로 학습 | 이전 모델의 성능 영향을 받음 |
| 목적 | 분산 감소 | 편향 감소 |
| 결과 결합 방식 | Voting | Weighted Averaging |
---
## 5. Gradient Boosting Machine (GBM)
- **작동 원리**:
    1. 잔차(Residual: Y−Prediction)를 예측하는 결정 트리 학습.
        Y−PredictionY - \text{Prediction}
    2. 각 리프 노드의 평균값을 계산.
    3. 새로운 모델로 예측값 갱신.
- **문제점**:
    - 모든 데이터와 피처를 스캔하여 그래디언트를 계산 → 높은 계산 비용.
    - 잔차가 0에 가까울 경우 과적합 발생 가능.
- **규제(Regularization)**:
    - **Subsampling**: 일부 데이터를 추출해 학습.
    - **Shrinkage**: 추가 모델의 영향력을 조정(learning rate로 조절).
    - **Early Stopping**: 과적합 방지를 위해 학습 조기 종료.
---
## 6. LightGBM
- **목적**:GBM의 계산 비용 문제와 대규모 데이터 학습의 한계를 극복.
- **핵심 기술**:
    - **GOSS (Gradient-based One-Side Sampling)**:
        - 그래디언트 값에 따라 데이터 샘플을 정렬하여 중요한 샘플(그래디언트 큰 값)을 우선적으로 학습.
        - 데이터 인스턴스 감소로 학습 효율 향상.
    - **EFB (Exclusive Feature Bundling)**:
        - 피처들 간의 충돌(conflict)을 최소화하면서 피처 수를 줄이는 방법.
        - 피처를 번들로 묶어 계산량 감소.
### EFB 세부 사항
- **충돌(conflict)**:
    - 두 피처가 동시에 0이 아닌 값을 가지는 경우.
- **파라미터**:
    - `max_conflict_count`:허용 가능한 최대 충돌 수. 값이 클수록 더 많은 피처를 묶을 수 있지만 정보 손실 우려.
---
## 7. LightGBM 주요 파라미터
### **1) 학습 전략**:
- `boosting`: 모델 학습 방식.
    - gbdt (default), rf, dart 등.
- `objective`: 학습 목표.
    - 회귀(`regression`), L1 손실(`regression_l1`), 교차 엔트로피(`cross_entropy`).
### **2) 과적합 방지**:
- `max_depth`: 트리의 최대 깊이.
- `num_leaves`: 리프 노드의 최대 개수.
- `min_data_in_leaf`: 각 리프에 포함된 최소 데이터 수.
### **3) 속도 향상**:
- `feature_fraction`: 피처 하위 집합 무작위 선택 비율.
- `bagging_fraction`: 데이터(row) 하위 집합 무작위 선택 비율.