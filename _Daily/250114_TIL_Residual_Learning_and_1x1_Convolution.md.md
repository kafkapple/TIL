# Residual_Learning_and_1x1_Convolution

# 📄 Residual_Learning_and_1x1_Convolution

ResNet의 핵심 개념인 Residual Learning과 1x1 Convolution은 깊은 신경망의 효율적인 학습을 가능하게 하는 중요한 기술입니다.

### 1. Residual Learning (잔차 학습)

#### 1.1 핵심 개념

- **잔차 함수 학습**: 네트워크가 원하는 함수

`H(x)H(x)H(x)`

를 직접 학습하는 대신, 입력

`xxx`

에 대한 잔차

`F(x)=H(x)−xF(x) = H(x) - xF(x)=H(x)−x`

를 학습.

- **Shortcut 연결**: 입력

`xxx`

를 다음 레이어에 직접 더하여

`H(x)=F(x)+xH(x) = F(x) + xH(x)=F(x)+x`

를 구성.

- **학습 난이도 감소**: "전체 함수" 대신 "입력 대비 보정값"만 학습하여 학습이 단순화되고 기울기 전파가 용이.

#### 1.2 장점 및 효과

- **기울기 소실 완화**: 깊은 네트워크에서 발생하는 기울기 소실 문제를 효과적으로 줄임.

- **학습 효율성 증대**: 네트워크가 더 깊어져도 안정적이고 빠르게 수렴 가능.

- **깊은 네트워크 구축**: 수백 개 이상의 레이어를 가진 매우 깊은 신경망 설계 및 학습 가능.

### 2. 1x1 Convolution (1x1 컨볼루션)

#### 2.1 정의 및 역할

- **채널 방향 연산**: 1x1 크기의 필터로 공간 정보(Height, Width)는 유지한 채 채널 방향으로만 연산 수행.

- **채널 간 선형 결합**: 각 픽셀 위치에서 채널별 가중합을 계산하여 채널 간의 선형 결합 수행.

- **차원 축소/확장**: 채널 수를 줄이거나 늘려 연산량 및 파라미터 수를 최적화.

#### 2.2 Bottleneck Block에서의 활용

- **구조**: "1x1 Conv → 3x3 Conv → 1x1 Conv"의 순서로 구성.

- **채널 축소**: 첫 번째 1x1 Conv로 채널을 축소하여 3x3 Conv의 연산량 감소.

- **채널 복원**: 마지막 1x1 Conv로 채널을 복원하여 정보 손실 최소화.

- **효율적인 깊이**: 적은 파라미터로 깊은 네트워크를 구축하여 연산 효율성 및 표현력 동시 확보.

### 3. 핵심 개념 비교

#### 3.1 목표 및 기능

- **Residual Learning**: 깊은 네트워크의 학습 난이도를 낮추고 기울기 소실 문제를 해결.

- **1x1 Convolution**: 채널 차원 조절을 통해 연산 효율성을 높이고 네트워크의 표현력을 강화.

#### 3.2 ResNet에서의 시너지

- **Residual Block**: 1x1 Conv를 포함한 Bottleneck Block이 Residual Learning과 결합되어 깊고 효율적인 ResNet 구조를 형성.

- **학습 안정성**: Residual Learning으로 학습 안정성을 확보하고, 1x1 Conv로 연산 효율성을 극대화.

- **성능 향상**: 두 기술의 결합을 통해 이미지 인식 등 다양한 분야에서 뛰어난 성능 달성.