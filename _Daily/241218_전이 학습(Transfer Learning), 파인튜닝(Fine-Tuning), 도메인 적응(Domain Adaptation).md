### 
---
### **1. 전이 학습 (Transfer Learning)**
- **정의**: 사전 학습된 모델에서 얻은 지식을 새로운 작업에 활용하는 학습 방법.
- **목적**: 이전 작업에서 학습한 일반적인 패턴(지식)을 사용하여 새로운, 종종 관련된 작업을 더 효율적으로 해결.
- **핵심 아이디어**:
    - 대규모 데이터셋(예: ImageNet)으로 학습한 일반적인 특징 추출 능력을 새로운 데이터셋에 적용.
- **활용 사례**:
    - **컴퓨터 비전**: 사전 학습된 CNN 모델을 새로운 이미지 데이터셋에 적용.
    - **자연어 처리(NLP)**: BERT나 GPT 모델을 활용해 텍스트 요약, 감성 분석 등 수행.
---
### **2. 파인튜닝 (Fine-Tuning)**
- **정의**: 전이 학습의 특화된 형태.
- **핵심 과정**:
    1. 사전 학습된(pretrained) 모델을 기반으로 시작.
    2. 필요에 따라 모델 구조를 수정(예: 새로운 레이어 추가).
    3. 특정 작업에 맞는 데이터로 모델을 추가 훈련.
- **특징**:
    - 사전 학습된 모델의 일부 레이어를 고정(freeze)하거나 선택적으로 고정 해제(unfreeze)하여 학습.
    - 기존의 일반적인 지식을 특정 작업 요구사항에 맞게 세부적으로 조정.
- **예시**:
    - 대형 NLP 모델을 특정 도메인(예: 의학, 법률) 데이터로 세부 튜닝.
---
### **3. 도메인 적응 (Domain Adaptation)**
- **정의**: 학습 데이터와 작업 데이터의 분포가 다를 때 모델을 새로운 도메인에 적응시키는 기법.
- **중점**: 서로 다른 데이터 분포(도메인) 간 차이를 줄여, 지식을 효과적으로 전이.
- **예시**:
    - 실세계 사진으로 학습된 모델을 만화 이미지에 적용.
- **기술적 접근**:
    - 특징 정렬(feature alignment).
    - 적대적 학습(adversarial training) 기반 손실 함수.
---
### **4. 관련 학습 방법**
1. **다중 작업 학습 (Multi-task Learning)**:
    - 여러 관련된 작업을 동시에 학습하며, 작업 간의 공통 표현을 공유.
    - 작업 간 상호 보완적 정보를 활용하여 일반화 성능 향상.
2. **제로샷 학습 (Zero-shot Learning)**:
    - 학습 중 보지 못한 클래스(레이블)를 예측.
    - 예시: CLIP 모델은 시각 및 텍스트 임베딩을 활용하여 학습되지 않은 새로운 클래스를 분류.
3. **원샷/소수 샷 학습 (One-shot/Few-shot Learning)**:
    - 단 하나 또는 소수의 레이블 데이터로부터 학습.
    - 주로 레이블이 부족한 경우 또는 메타 학습(meta-learning) 상황에서 활용.