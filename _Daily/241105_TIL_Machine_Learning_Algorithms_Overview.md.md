선형 회귀, KNN, 의사결정 나무, 랜덤 포레스트 등 주요 머신러닝 알고리즘의 핵심 개념, 작동 원리, 장단점 및 개선 방안을 체계적으로 정리

### 1. 선형 회귀 (Linear Regression)

#### 1.1 기본 개념 및 가정

- **정의**: 독립 변수와 종속 변수 간의 선형 관계 모델링

- **최소자승법(OLS)**: 잔차 제곱합 최소화를 통한 최적 직선 도출

- **주요 가정**: 선형성, 독립성, 등분산성, 잔차의 정규성

#### 1.2 모델 평가 및 해석

- **잔차 분석**: 등분산성(잔차 vs 적합값), 정규성(Q-Q 플롯), 자기상관(Durbin-Watson) 확인

- **다중공선성**: 상관계수 행렬, VIF(분산팽창계수)를 통한 진단

- **모델 해석**: 회귀 계수(변수 영향), 결정계수(R-squared, 설명력), 조정된 결정계수(Adjusted R-squared)

#### 1.3 장단점 및 개선

- **장점**: 해석 용이성, 계산 효율성

- **단점**: 선형 관계 가정, 이상치 민감, 다중공선성 문제

- **개선**: 변수 변환, 변수 선택, 정규화/표준화

### 2. K-최근접 이웃 (K-Nearest Neighbors, KNN)

#### 2.1 정의 및 특징

- **정의**: K개의 가장 가까운 이웃 데이터를 기반으로 예측하는 비모수적 알고리즘

- **특징**: 사례 기반 학습, 거리 기반 모델, 분류/회귀 모두 적용 가능

- **거리 측정**: 유클리드 거리, 맨해튼 거리 등 활용

#### 2.2 장단점 및 고려 사항

- **장점**: 단순성, 직관성, 모델 가정 불필요

- **단점**: 높은 계산 복잡도(대용량 데이터), 스케일 민감도, 차원의 저주

- **주요 고려 사항**: K 값 선택(과적합/과소적합), 거리 측정 방법, 데이터 전처리(스케일링, 결측치)

#### 2.3 개선 방법

- **가중치 부여**: 이웃 데이터에 가중치 적용

- **차원 축소**: PCA 등 기법 활용

- **효율적인 데이터 구조**: KD-트리, Ball-트리 등을 통한 검색 시간 단축

### 3. 의사결정 나무 (Decision Tree)

#### 3.1 정의 및 작동 원리

- **정의**: 데이터 특성 기반 의사결정 규칙을 트리 형태로 생성하여 분류/회귀 수행

- **작동 원리**: 분할 기준 설정, 재귀적 분할, 종료 조건

- **분할 기준**: 불순도(지니 지수, 엔트로피), 정보 이득

#### 3.2 주요 하이퍼파라미터

- **max_depth**: 트리의 최대 깊이 제한

- **min_samples_split**: 노드 분할을 위한 최소 샘플 수

- **min_samples_leaf**: 리프 노드의 최소 샘플 수

- **max_features**: 분할에 사용할 최대 특성 수

#### 3.3 장단점 및 개선

- **장점**: 해석 용이성, 전처리 불필요, 다양한 데이터 처리

- **단점**: 과적합 위험, 불안정성, 일반화 어려움

- **개선**: 가지치기(Pruning), 앙상블 기법(랜덤 포레스트, 그래디언트 부스팅), 균형 잡힌 데이터 사용

### 4. 랜덤 포레스트 (Random Forest)

#### 4.1 정의 및 작동 원리

- **정의**: 배깅과 특성 무작위 선택을 결합한 앙상블 학습 방법

- **작동 원리**: 부트스트랩 샘플링, 무작위 특성 선택, 예측 통합(분류: 투표, 회귀: 평균)

- **구성 요소**: 여러 개의 결정 트리

#### 4.2 장단점 및 하이퍼파라미터

- **장점**: 높은 정확도, 과적합 방지, 특성 중요도 제공

- **단점**: 높은 계산 비용, 해석 어려움

- **주요 하이퍼파라미터**: n_estimators(트리 수), max_features(특성 수), max_depth 등

#### 4.3 개선 방법

- **하이퍼파라미터 튜닝**: 교차 검증, 그리드 서치

- **병렬 처리**: 트리 생성 과정 속도 향상

- **특성 선택**: 중요도 낮은 특성 제거를 통한 효율성 향상