## AmadeusGPT 이중 메모리 시스템 심층 분석 - 메모리 & 검색 강화

## 핵심 개념 1: AmadeusGPT의 이중 메모리 시스템 ⭐⭐⭐

- 정의: LLM의 제한된 컨텍스트 창으로 인한 '망각' 문제를 해결하고, 장기적인 대화 연속성을 보장하기 위해 설계된 아키텍처

- 중요성: 복잡한 행동 규칙, 변수 등 핵심 정보의 소실을 방지하고, 세션 재시작 시 작업 상태를 복원하여 장기 상호작용의 일관성 유지 

- 세부:

- 단기 기억 (Short-Term Memory) ⭐⭐:

- 구현: 동적 데크(dynamic deque) 자료 구조

- 작동: 최신 대화 내용을 임시 저장하며, 토큰 한계 도달 시 오래된 내용 순차 삭제. 매 요청 시 LLM에 전달되어 현재 맥락 형성.

- 역할: 현재 진행 중인 대화의 흐름 유지 및 즉각적인 상호작용 담당.

- 장기 기억 (Long-Term Memory) ⭐⭐⭐:

- 구현: RAM 상주 딕셔너리 형태, 필요시 디스크 저장으로 영속성 확보.

- 쓰기: <|심볼_이름|> 사용 시 해당 맥락 전체를 '심볼_이름'으로 저장 (예: Define <|head_dips|>).

- 읽기: <심볼_이름> 사용 시 저장된 정보를 단기 기억 맨 앞에 삽입, LLM이 핵심 전제로 인식.

- 역할: 복잡한 행동 정의, 중요 변수, 반복 사용 규칙 등 핵심 정보 보호 및 세션 간 영속성 제공.

## 핵심 개념 2: 최신 LLM 메모리 활용 패러다임 ⭐⭐⭐

- 개요: AmadeusGPT 이중 메모리 시스템의 기반이 되는 두 가지 주요 LLM 메모리 활용 패러다임과 최적 활용 전략. 

- 특징:

- 메모리 증강 생성 (Memory-Augmented Generation, MAG) ⭐⭐⭐:

- 정의: 대화의 내부적, 동적인 맥락(이전 상호작용 기록)을 보존하여 연속성과 개인화를 목표. AmadeusGPT 이중 메모리 시스템과 일치.

- 최적 활용법:

- 복잡한 규칙/상태 저장: 분석 중 생성되는 복잡한 행동 정의(chase, huddle)나 ROI 설정값 등을 <|심볼_이름|>으로 장기 기억에 명시적 저장하여 분석 일관성 유지 및 반복 작업 감소.

- 점진적 분석 구축: 간단한 분석 후 결과를 확인하고, 저장된 심볼을 <>로 불러와 더 복잡한 분석을 점진적으로 구축 (예: <chase> 이벤트 정의 후 "Plot trajectory during <chase>").

- 검색 증강 생성 (Retrieval-Augmented Generation, RAG) ⭐⭐⭐:

- 정의: LLM이 응답 생성 전, 위키피디아, 내부 문서 등 **외부의 정적인 지식 소스**를 검색하여 답변의 사실적 정확성과 신뢰도 향상. '환각' 현상 감소에 효과적.

- 최적 활용법 (AmadeusGPT와 결합 시):

- 과학적 근거 확보: "의학 문헌에서 '생쥐의 얼어붙는 행동' 정의 기준은?" 질문으로 RAG가 관련 논문 검색, 답변 제공 후 AmadeusGPT에 정확한 분석 명령.

- 내부 문서/코드 검색: "우리 연구실의 표준 EPM 분석 프로토콜은?" 질문으로 내부 데이터베이스 검색, 분석 방향 설정 및 필요 정보 획득.

- 문제 해결/고객 지원: "L2 고객 지원팀의 자주 발생하는 문제 유형은?" 질문으로 과거 지원 티켓 검색, 문제 해결 우선순위 설정.

## 연결고리 ⭐⭐⭐

- MAG ↔ RAG: AmadeusGPT의 이중 메모리 시스템(MAG)은 LLM의 내부적인 대화 맥락을 기억하게 하고, RAG는 외부의 검증된 지식을 통합하여 답변의 정확성을 높임.

- 전체 흐름: **하이브리드 워크플로우**는 RAG를 통해 분석의 사실적 기반을 다지고, MAG를 통해 얻은 지식을 바탕으로 정교한 분석 규칙을 정의 및 저장하며, 저장된 규칙을 불러와 현재 분석에 적용함으로써 객관적 사실과 대화의 고유 맥락이 결합된 신뢰도 높은 결과물을 생성함.