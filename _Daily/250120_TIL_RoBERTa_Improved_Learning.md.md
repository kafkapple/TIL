# 📄 RoBERTa_Improved_Learning

RoBERTa는 NSP 제거 및 학습 전략 개선을 통해 BERT 대비 성능을 향상시킨 언어 모델입니다.

### 1. RoBERTa 핵심 개선 사항

#### 1.1 NSP 제거

- 학습 단순화

- MLM 학습 효율성 극대화

- 불필요한 제약 해소

#### 1.2 학습 데이터 확장

- CommonCrawl, OpenWebText, Wikipedia 등 대규모 데이터셋 활용

- 160GB 데이터 사용

- 풍부한 학습 데이터 확보

#### 1.3 동적 마스킹

- 매 iteration마다 새로운 마스킹 패턴 적용

- 데이터 다양성 극대화

- 모델 일반화 능력 향상

### 2. RoBERTa 학습 전략

#### 2.1 배치 크기 및 학습 시간

- 최대 8192 배치 크기

- 학습 안정성 및 모델 수렴 개선

- BERT 대비 학습 시간 연장

#### 2.2 데이터 순서 임의화

- 데이터 샘플링 다양화

- 문맥 정보 학습 강화

- 과적합 방지

#### 2.3 암묵적 문장 관계 학습

- NSP 없이 문장 간 관계 학습

- 대규모 데이터 및 전략적 학습 활용

- 텍스트 표현 학습 강화

### 3. RoBERTa 성과 및 의의

#### 3.1 MLM 중심 학습

- NSP 제거 후 MLM 성능 극대화

- 대규모 데이터 및 개선된 학습 전략

- 언어 모델 학습의 새로운 방향 제시

#### 3.2 BERT 대비 성능 우위

- 더 강력한 텍스트 표현 학습

- 문장 간 관계 암묵적 학습

- 다양한 NLP 태스크에서 높은 성능 달성

|개선 요소|설명|
|---|---|
|**1. 더 큰 데이터 세트**|CommonCrawl, OpenWebText, Wikipedia 등 **160GB 데이터 사용**으로 더 풍부한 학습 데이터 확보.|
|**2. 동적 마스킹**|학습 중 매 iteration마다 새로운 마스킹 패턴 적용으로 데이터 다양성 극대화.|
|**3. 긴 배치 크기**|최대 **8192** 배치 크기로 학습 안정성과 모델 수렴 개선.|
|**4. 더 긴 학습 시간**|BERT보다 학습 시간 연장으로 대규모 데이터 심층 학습.|
|**5. 데이터 순서 임의화**|데이터 샘플링 다양화로 문맥 정보 학습 강화.|
|**6. 암묵적 문장 관계 학습**|NSP 없이도 대규모 데이터와 전략적 학습으로 문장 간 관계를 효과적으로 학습.|